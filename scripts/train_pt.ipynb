{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "! pip install -q bitsandbytes deepspeed huggingface_hub\n",
    "! pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model and dataset\n",
    "model_name_or_path = \"stabilityai/japanese-stablelm-base-gamma-7b\"\n",
    "dataset = \"wikipedia_ja_2022,slim_pajama_en,culturax_ja,wikipedia_en_2022,code_stack_en\"\n",
    "\n",
    "# Training parameters\n",
    "mix_strategy = \"interleave_over\"\n",
    "max_length = 4096\n",
    "lora_rank = 64\n",
    "lora_alpha = 128.0\n",
    "lora_dropout = 0.05\n",
    "lora_target = \"q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj\"\n",
    "additional_target = \"lm_head,embed_tokens\"\n",
    "output_dir = \"./pt_outputs\"\n",
    "logging_dir = \"./pt_logs\"\n",
    "overwrite_output_dir = True\n",
    "streaming = True\n",
    "max_steps = int(2.5e5)\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 2\n",
    "lr_scheduler_type = \"cosine\"\n",
    "logging_steps = 10\n",
    "save_steps = 200\n",
    "learning_rate = 2e-4\n",
    "num_train_epochs = 1.0\n",
    "plot_loss = True\n",
    "\n",
    "# performance parameters\n",
    "use_flash_attn = True\n",
    "use_bf16 = True\n",
    "use_fp16 = True\n",
    "\n",
    "if use_bf16:\n",
    "    use_fp16 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "! python src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --do_train \\\n",
    "    --dataset {dataset} \\\n",
    "    --template default \\\n",
    "    --mix_strategy {mix_strategy} \\\n",
    "    --max_length {max_length} \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_rank {lora_rank} \\\n",
    "    --lora_alpha {lora_alpha} \\\n",
    "    --lora_dropout {lora_dropout} \\\n",
    "    --lora_target {lora_target} \\\n",
    "    --additional_target {additional_target} \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --logging_dir {logging_dir} \\\n",
    "    {'--overwrite_cache' if overwrite_cache else ''} \\\n",
    "    {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "    {'--streaming' if streaming else ''} \\\n",
    "    --max_steps {max_steps} \\\n",
    "    --per_device_train_batch_size {per_device_train_batch_size} \\\n",
    "    --gradient_accumulation_steps {gradient_accumulation_steps} \\\n",
    "    --lr_scheduler_type {lr_scheduler_type} \\\n",
    "    --logging_steps {logging_steps} \\\n",
    "    --save_steps {save_steps} \\\n",
    "    --learning_rate {learning_rate} \\\n",
    "    --num_train_epochs {num_train_epochs} \\\n",
    "    --plot_loss \\\n",
    "    {'--plot_loss' if plot_loss else ''} \\\n",
    "    {'--flash_attn' if use_flash_attn else ''} \\\n",
    "    {'--bf16' if use_bf16 else ''} \\\n",
    "    {'--fp16' if use_fp16 else ''}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
